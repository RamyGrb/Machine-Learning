{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Identification\n",
    "\n",
    "Author: Ramy Ghorayeb\n",
    "    \n",
    "Access the competition and download the dataset [here](https://www.kaggle.com/c/spooky-author-identification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('data/train.zip',nrows=1000,compression='zip')\n",
    "data_test = pd.read_csv('data/test.zip',compression='zip')\n",
    "data_sample = pd.read_csv('data/sample_submission.zip',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      "id        1000 non-null object\n",
      "text      1000 non-null object\n",
      "author    1000 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "enc = preprocessing.LabelEncoder()\n",
    "y = enc.fit_transform(data['author'])\n",
    "x = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# cross validation\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y,\n",
    "                                                stratify=y,\n",
    "                                                test_size=0.1,\n",
    "                                                shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parametrize the distribution of words by TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=2, \n",
    "                      token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1, 3),\n",
    "                      use_idf=1,\n",
    "                      smooth_idf=1,\n",
    "                      sublinear_tf=1,\n",
    "                      stop_words = 'english')\n",
    "\n",
    "tfv.fit(list(xtrain) + list(xtest))\n",
    "\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xtest_tfv = tfv.transform(xtest)\n",
    "\n",
    "tfidf_weight = dict(zip(tfv.get_feature_names(), tfv.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.8956332500635316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "lg = LogisticRegression(C=1.0,multi_class='auto',solver='newton-cg')\n",
    "lg.fit(xtrain_tfv, ytrain)\n",
    "\n",
    "y_lg = lg.predict_proba(xtest_tfv)\n",
    "\n",
    "print('loss: ', log_loss(ytest,y_lg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import progressbar\n",
    "\n",
    "def log_test(x,y,model,n=100):\n",
    "    model_loss_train = []\n",
    "    model_loss_test = []    \n",
    "    for r in progressbar.progressbar(range(n)):\n",
    "        xtrain,xtest,ytrain,ytest = train_test_split(data['text'],y,\n",
    "                                                stratify=y,\n",
    "                                                test_size=0.1,\n",
    "                                                shuffle=True)\n",
    "        \n",
    "        xtrain_tfv =  tfv.transform(xtrain) \n",
    "        xtest_tfv = tfv.transform(xtest)\n",
    "        \n",
    "        model.fit(xtrain_tfv,ytrain)\n",
    "        \n",
    "        y_mod_train = model.predict_proba(xtrain_tfv)\n",
    "        y_mod_test = model.predict_proba(xtest_tfv)\n",
    "        model_loss_train.append(log_loss(ytrain,y_mod_train))\n",
    "        model_loss_test.append(log_loss(ytest,y_mod_test))\n",
    "        \n",
    "    return model_loss_train, model_loss_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  3., 30., 51., 16.])],\n",
       " array([0.58794057, 0.62676966, 0.66559875, 0.70442783, 0.74325692,\n",
       "        0.782086  , 0.82091509, 0.85974418, 0.89857326, 0.93740235,\n",
       "        0.97623144]),\n",
       " <a list of 2 Lists of Patches objects>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_train, loss_test = log_test(x,y,lg)\n",
    "\n",
    "plt.hist([loss_train,loss_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., 14., 62., 24.])],\n",
       " array([0.58496942, 0.62163052, 0.65829162, 0.69495272, 0.73161382,\n",
       "        0.76827492, 0.80493602, 0.84159712, 0.87825822, 0.91491932,\n",
       "        0.95158043]),\n",
       " <a list of 2 Lists of Patches objects>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "loss_train, loss_test = log_test(x,y,nb)\n",
    "plt.hist([loss_train,loss_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (10 of 10) |########################| Elapsed Time: 0:00:15 Time:  0:00:15\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "loss_train, loss_test = log_test(x,y,xb,n=10)\n",
    "plt.hist([loss_train,loss_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  -0.7997231930507456\n",
      "Best parameters set: {'alpha': 0.109, 'class_prior': None, 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import metrics\n",
    "\n",
    "ll = metrics.make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "param_grid = {'alpha': np.linspace(0.01,1,21)}\n",
    "\n",
    "model = GridSearchCV(estimator=nb, param_grid=param_grid,scoring=ll,cv=5)\n",
    "\n",
    "model.fit(xtrain_tfv,ytrain)\n",
    "\n",
    "print('Best score: ', model.best_score_)\n",
    "print(\"Best parameters set:\", model.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('data/train.zip',nrows=1000,compression='zip')\n",
    "data_test = pd.read_csv('data/test.zip',compression='zip')\n",
    "data_sample = pd.read_csv('data/sample_submission.zip',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc = preprocessing.LabelEncoder()\n",
    "y = enc.fit_transform(data['author'])\n",
    "x = data['text']\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y,\n",
    "                                                stratify=y,\n",
    "                                                test_size=0.1,\n",
    "                                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |                                 #            | 399999 Elapsed Time: 0:00:30\n"
     ]
    }
   ],
   "source": [
    "import progressbar \n",
    "\n",
    "def load_emb(loc):\n",
    "    index = {}\n",
    "    f = open(loc)\n",
    "    for line in progressbar.progressbar(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:],dtype='float32')\n",
    "        index[word] = coefs\n",
    "    f.close()\n",
    "    return index\n",
    "\n",
    "glove = load_emb('glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "    \n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()] #we choose to remove special characters\n",
    "    M = []\n",
    "    \n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtrain_emb = [sent2vec(x) for x in xtrain]\n",
    "xtest_emb = [sent2vec(x) for x in xtest]\n",
    "\n",
    "xtrain_emb = np.array(xtrain_emb)\n",
    "xtest_emb = np.array(xtest_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline shallow modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################| Elapsed Time: 0:00:03 Time:  0:00:03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  1., 17., 54., 28.])],\n",
       " array([0.58563215, 0.62196727, 0.65830239, 0.69463751, 0.73097262,\n",
       "        0.76730774, 0.80364286, 0.83997797, 0.87631309, 0.91264821,\n",
       "        0.94898332]),\n",
       " <a list of 2 Lists of Patches objects>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADklJREFUeJzt3X+s3fVdx/Hna1SGmwKFXgi2sHam4JiJYbkhuEW3gJEf01EVTFFnxWoTM3GKxjH5g2aJCUvMQJNlpoFtddlgiEsgihrSgcbFES+/xo8KrQWhtNI7B/Vn3Jhv/zjf6qXecm/P95x7z/3k+Uhuzvf7Od/vOa9+e/q63/v53nOaqkKS1K43LXcASdJ4WfSS1DiLXpIaZ9FLUuMseklqnEUvSY2z6CWpcRa9JDXOopekxq1a7gAAa9asqfXr1y93DElaUR5++OGvV9XUQttNRNGvX7+emZmZ5Y4hSStKkn9czHZO3UhS4yx6SWqcRS9JjbPoJalxFr0kNW7Bok/y6SSHkjw5Z+y0JPcn2dPdru7Gk+QPkuxN8rUk7xpneEnSwhZzRv9Z4LKjxm4AdlXVRmBXtw5wObCx+9oGfGo0MSVJw1qw6Kvqr4FvHDV8JbCzW94JbJoz/kc18FXg1CRnjSqsJOn4DTtHf2ZVHQTobs/oxtcCL87Zbn83JklaJqN+Z2zmGZv3fx9Pso3B9A7nnHPO0E+4/oY/O+59nr/5/UM/nyStNMOe0b98ZEqmuz3Uje8Hzp6z3TrgwHwPUFU7qmq6qqanphb8qAZJ0pCGLfp7gS3d8hbgnjnjP9/99s1FwOEjUzySpOWx4NRNkjuA9wFrkuwHbgJuBu5KshV4Abi62/w+4ApgL/AfwLVjyCxJOg4LFn1VXXOMuy6ZZ9sCPtQ3lCRpdHxnrCQ1zqKXpMZZ9JLUOItekhpn0UtS4yx6SWqcRS9JjbPoJalxFr0kNc6il6TGWfSS1DiLXpIaZ9FLUuMseklqnEUvSY2z6CWpcRa9JDXOopekxln0ktQ4i16SGmfRS1LjLHpJapxFL0mNs+glqXEWvSQ1zqKXpMZZ9JLUOItekhpn0UtS4yx6SWqcRS9JjbPoJalxvYo+yW8keSrJk0nuSHJSkg1JHkqyJ8kXk5w4qrCSpOM3dNEnWQv8GjBdVd8PnABsBj4O3FJVG4FXgK2jCCpJGk7fqZtVwHcmWQW8BTgIXAzc3d2/E9jU8zkkST0MXfRV9RLwe8ALDAr+MPAw8GpVvdZtth9Y2zekJGl4faZuVgNXAhuA7wHeClw+z6Z1jP23JZlJMjM7OztsDEnSAvpM3fwI8FxVzVbVt4AvAe8GTu2mcgDWAQfm27mqdlTVdFVNT01N9YghSXojfYr+BeCiJG9JEuAS4GngAeCqbpstwD39IkqS+ugzR/8Qg4uujwBPdI+1A/gIcH2SvcDpwO0jyClJGtKqhTc5tqq6CbjpqOF9wIV9HleSNDq+M1aSGmfRS1LjLHpJapxFL0mNs+glqXEWvSQ1zqKXpMZZ9JLUOItekhpn0UtS4yx6SWqcRS9JjbPoJalxFr0kNc6il6TGWfSS1DiLXpIaZ9FLUuMseklqnEUvSY2z6CWpcRa9JDXOopekxln0ktQ4i16SGmfRS1LjLHpJapxFL0mNs+glqXEWvSQ1zqKXpMZZ9JLUOItekhrXq+iTnJrk7iR/n2R3kh9MclqS+5Ps6W5XjyqsJOn49T2j/33gL6rq+4AfAHYDNwC7qmojsKtblyQtk6GLPsnJwA8DtwNU1Ter6lXgSmBnt9lOYFPfkJKk4fU5o387MAt8JsmjSW5L8lbgzKo6CNDdnjGCnJKkIfUp+lXAu4BPVdUFwL9zHNM0SbYlmUkyMzs72yOGJOmN9Cn6/cD+qnqoW7+bQfG/nOQsgO720Hw7V9WOqpququmpqakeMSRJb2Tooq+qfwJeTHJeN3QJ8DRwL7ClG9sC3NMroSSpl1U9978O+HySE4F9wLUMvnnclWQr8AJwdc/nkCT10Kvoq+oxYHqeuy7p87iSpNHxnbGS1DiLXpIaZ9FLUuMseklqnEUvSY2z6CWpcRa9JDWu7xumJGmybD9liH0Ojz7HBPGMXpIaZ9FLUuMseklqnEUvSY2z6CWpcRa9JDXOopekxln0ktQ4i16SGmfRS1LjLHpJapxFL0mNs+glqXEWvSQ1zqKXpMZZ9JLUOItekhpn0UtS4yx6SWqcRS9JjbPoJalxFr0kNc6il6TGWfSS1LjeRZ/khCSPJvnTbn1DkoeS7EnyxSQn9o8pSRrWKM7oPwzsnrP+ceCWqtoIvAJsHcFzSJKG1Kvok6wD3g/c1q0HuBi4u9tkJ7Cpz3NIkvrpe0Z/K/DbwH9366cDr1bVa936fmBtz+eQJPUwdNEn+THgUFU9PHd4nk3rGPtvSzKTZGZ2dnbYGJKkBfQ5o38P8IEkzwN3MpiyuRU4Ncmqbpt1wIH5dq6qHVU1XVXTU1NTPWJIkt7I0EVfVR+tqnVVtR7YDHy5qn4WeAC4qttsC3BP75SSpKGN4/foPwJcn2Qvgzn728fwHJKkRVq18CYLq6oHgQe75X3AhaN4XElSf74zVpIaZ9FLUuMseklqnEUvSY2z6CWpcRa9JDXOopekxln0ktQ4i16SGmfRS1LjLHpJapxFL0mNs+glqXEWvSQ1zqKXpMZZ9JLUOItekhpn0UtS4yx6SWqcRS9JjRvJfw4uSQK2nzLEPodHn+MontFLUuMseklqnEUvSY2z6CWpcRa9JDXOopekxln0ktQ4i16SGmfRS1LjLHpJapxFL0mNs+glqXFDF32Ss5M8kGR3kqeSfLgbPy3J/Un2dLerRxdXknS8+pzRvwb8ZlW9A7gI+FCS84EbgF1VtRHY1a1LkpbJ0EVfVQer6pFu+V+B3cBa4EpgZ7fZTmBT35CSpOGNZI4+yXrgAuAh4MyqOgiDbwbAGcfYZ1uSmSQzs7Ozo4ghSZpH76JP8l3AnwC/XlX/stj9qmpHVU1X1fTU1FTfGJKkY+hV9Em+g0HJf76qvtQNv5zkrO7+s4BD/SJKkvro81s3AW4HdlfVJ+bcdS+wpVveAtwzfDxJUl99/s/Y9wAfBJ5I8lg39jvAzcBdSbYCLwBX94soSepj6KKvqr8Bcoy7Lxn2cSVJo+U7YyWpcX2mbiRpfttPGWKfw6PPIcAzeklqnkUvSY2z6CWpcRa9JDXOopekxln0ktQ4i16SGmfRS1LjLHpJapxFL0mNs+glqXEWvSQ1zqKXpMZZ9JLUOItekhpn0UtS4yx6SWqcRS9JjbPoJalxFr0kNc6il6TGWfSS1DiLXpIaZ9FLUuMseklqnEUvSY2z6CWpcRa9JDXOopekxln0ktS4sRR9ksuSPJNkb5IbxvEckqTFGXnRJzkB+CRwOXA+cE2S80f9PJKkxVk1hse8ENhbVfsAktwJXAk8PYbnknQs2085zu0PjyeHlt04pm7WAi/OWd/fjUmSlkGqarQPmFwNXFpVv9StfxC4sKquO2q7bcC2bvU84JmRBnm9NcDXx/j4o2TW8VgpWVdKTjDruBxP1rdV1dRCG41j6mY/cPac9XXAgaM3qqodwI4xPP//k2SmqqaX4rn6Mut4rJSsKyUnmHVcxpF1HFM3fwdsTLIhyYnAZuDeMTyPJGkRRn5GX1WvJflV4C+BE4BPV9VTo34eSdLijGPqhqq6D7hvHI89pCWZIhoRs47HSsm6UnKCWcdl5FlHfjFWkjRZ/AgESWrcii/6xXzcQpKfTvJ0kqeSfGHO+JYke7qvLROe9dtJHuu+xn5xe6GsSW6Zk+fZJK/OuW/JjmvPnJN2TM9J8kCSR5N8LckVc+77aLffM0kundSsSdYn+c85x/UPlznn25Ls6jI+mGTdnPsm6t//Aln7vVarasV+MbjY+w/A24ETgceB84/aZiPwKLC6Wz+juz0N2Nfdru6WV09i1m753ybpuB61/XUMLrov6XHtk3MSjymDudlf6ZbPB56fs/w48GZgQ/c4J0xo1vXAkxN0TP8Y2NItXwx8bqlfp32zjuK1utLP6P/34xaq6pvAkY9bmOuXgU9W1SsAVXWoG78UuL+qvtHddz9w2YRmXWqLyTrXNcAd3fJSHtc+OZfaYrIWcHK3fAr/9/6TK4E7q+q/quo5YG/3eJOYdSktJuf5wK5u+YE590/iv/9jZe1tpRf9Yj5u4Vzg3CRfSfLVJJcdx76j1CcrwElJZrrxTWPMudiswODHTQZnmV8+3n1HoE9OmLxjuh34uST7GfzW2pF3k0/ia3U782cF2NBN6fxVkh9a5pyPAz/VLf8E8N1JTl/kvqPUJyv0fK2u9KLPPGNH/xrRKgZTIu9jcEZ3W5JTF7nvKPXJCnBODd4t9zPArUm+d1xBOb5jsxm4u6q+PcS+ffXJCZN3TK8BPltV64ArgM8ledMi9x2lPlkPMjiuFwDXA19IcjLjsZicvwW8N8mjwHuBl4DXFrnvKPXJCj1fqyu96BfzcQv7gXuq6lvdj73PMCjTRX1Uw4RkpaoOdLf7gAeBC5Y56xGbef10yFIe1z45J/GYbgXu6jL9LXASg889mcTX6rxZu+mlf+7GH2YwL33ucuWsqgNV9ZPdN54bu7HDi9l3grL2f62O6+LDUnwxOAPex+BH8iMXON551DaXATu75TUMfnw6ncFFmOcYXIhZ3S2fNqFZVwNvnjO+hze46LgUWbvtzgOep3s/Rje2ZMe1Z86JO6bAnwO/0C2/g0ERBHgnr78Yu4/xXoztk3XqSDYGFx5fWs6//+7v9k3d8u8CH1vq1+kIsvZ+rY7lD7WUXwx+bHyWwZnDjd3Yx4APdMsBPsHg8/CfADbP2fcXGVzY2gtcO6lZgXd36493t1uXO2u3vh24eZ59l+y4DptzEo8pg4txX+kyPQb86Jx9b+z2ewa4fFKzMphjfqobfwT48WXOeVVXjM8Ct9EV5lK/TvtkHcVr1XfGSlLjVvocvSRpARa9JDXOopekxln0ktQ4i16SGmfRS1LjLHpJapxFL0mN+x9fKZFcGXnkfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "loss_train, loss_test = log_test(x,y,nb)\n",
    "plt.hist([loss_train,loss_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Deep Learning modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#normalize\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_emb_scl = scl.fit_transform(xtrain_emb)\n",
    "xtest_emb_scl = scl.transform(xtest_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "#one-hot\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "ytest_enc = np_utils.to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_76 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 3)                 903       \n",
      "=================================================================\n",
      "Total params: 183,903\n",
      "Trainable params: 182,703\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,BatchNormalization,Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 1.0904 - acc: 0.4089 - val_loss: 1.0851 - val_acc: 0.4100\n",
      "Epoch 2/20\n",
      "900/900 [==============================] - 1s 601us/step - loss: 1.0863 - acc: 0.4089 - val_loss: 1.0852 - val_acc: 0.4100\n",
      "Epoch 3/20\n",
      "900/900 [==============================] - 1s 758us/step - loss: 1.0857 - acc: 0.4089 - val_loss: 1.0851 - val_acc: 0.4100\n",
      "Epoch 4/20\n",
      "900/900 [==============================] - 1s 601us/step - loss: 1.0858 - acc: 0.4089 - val_loss: 1.0852 - val_acc: 0.4100\n",
      "Epoch 5/20\n",
      "900/900 [==============================] - 1s 619us/step - loss: 1.0865 - acc: 0.4089 - val_loss: 1.0851 - val_acc: 0.4100\n",
      "Epoch 6/20\n",
      "900/900 [==============================] - 1s 777us/step - loss: 1.0881 - acc: 0.4089 - val_loss: 1.0852 - val_acc: 0.4100\n",
      "Epoch 7/20\n",
      "900/900 [==============================] - 1s 625us/step - loss: 1.0857 - acc: 0.4089 - val_loss: 1.0852 - val_acc: 0.4100\n",
      "Epoch 8/20\n",
      "900/900 [==============================] - 1s 754us/step - loss: 1.0872 - acc: 0.4089 - val_loss: 1.0851 - val_acc: 0.4100\n",
      "Epoch 9/20\n",
      "900/900 [==============================] - 1s 629us/step - loss: 1.0888 - acc: 0.4089 - val_loss: 1.0851 - val_acc: 0.4100\n",
      "Epoch 10/20\n",
      "900/900 [==============================] - 1s 617us/step - loss: 1.0860 - acc: 0.4089 - val_loss: 1.0857 - val_acc: 0.4100\n",
      "Epoch 11/20\n",
      "900/900 [==============================] - 1s 761us/step - loss: 1.0872 - acc: 0.4089 - val_loss: 1.0856 - val_acc: 0.4100\n",
      "Epoch 12/20\n",
      "900/900 [==============================] - 1s 600us/step - loss: 1.0861 - acc: 0.4089 - val_loss: 1.0852 - val_acc: 0.4100\n",
      "Epoch 13/20\n",
      "900/900 [==============================] - 1s 594us/step - loss: 1.0857 - acc: 0.4089 - val_loss: 1.0851 - val_acc: 0.4100\n",
      "Epoch 14/20\n",
      "900/900 [==============================] - 1s 752us/step - loss: 1.0860 - acc: 0.4089 - val_loss: 1.0853 - val_acc: 0.4100\n",
      "Epoch 15/20\n",
      "900/900 [==============================] - 1s 605us/step - loss: 1.0854 - acc: 0.4089 - val_loss: 1.0851 - val_acc: 0.4100\n",
      "Epoch 16/20\n",
      "900/900 [==============================] - 1s 608us/step - loss: 1.0880 - acc: 0.4089 - val_loss: 1.0855 - val_acc: 0.4100\n",
      "Epoch 17/20\n",
      "900/900 [==============================] - 1s 761us/step - loss: 1.0855 - acc: 0.4089 - val_loss: 1.0851 - val_acc: 0.4100\n",
      "Epoch 18/20\n",
      "900/900 [==============================] - 1s 606us/step - loss: 1.0866 - acc: 0.4089 - val_loss: 1.0852 - val_acc: 0.4100\n",
      "Epoch 19/20\n",
      "900/900 [==============================] - 1s 605us/step - loss: 1.0863 - acc: 0.4089 - val_loss: 1.0851 - val_acc: 0.4100\n",
      "Epoch 20/20\n",
      "900/900 [==============================] - 1s 773us/step - loss: 1.0873 - acc: 0.4089 - val_loss: 1.0852 - val_acc: 0.4100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a84092ba8>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_emb_scl, y=ytrain_enc, batch_size=32, \n",
    "          epochs=20, verbose=1, \n",
    "          validation_data=(xtest_emb_scl, ytest_enc)\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x,length='fixed',nmax=150):\n",
    "    # you can choose if the length of the vector is fixed to a certain number of tokens or just the max\n",
    "    list_s = list(x)    \n",
    "    if length == 'max':\n",
    "        nmax = max([len(word_tokenize(s)) for s in list_s])           \n",
    "\n",
    "    x_token = np.zeros((len(x),nmax,300))    \n",
    "        \n",
    "    for i,s in enumerate(list_s):\n",
    "        k=0\n",
    "        words_new = np.zeros((nmax,300)) \n",
    "        \n",
    "        words = str(s).lower()\n",
    "        words = word_tokenize(words)\n",
    "        \n",
    "        for w in words:\n",
    "            if k<nmax:\n",
    "                if w in glove.keys() and w not in stop_words:\n",
    "                    words_new[k] = glove[w]\n",
    "                    glove[w]\n",
    "                    k += 1\n",
    "                    \n",
    "        x_token[i] = words_new\n",
    "        \n",
    "    return x_token\n",
    "\n",
    "def tokenize_weighted(x,length='fixed',nmax=150):\n",
    "    # you can choose if the length of the vector is fixed to a certain number of tokens or just the max\n",
    "    list_s = list(x)\n",
    "    if length == 'max':\n",
    "        nmax = max([len(word_tokenize(s)) for s in list_s])  \n",
    "        \n",
    "    x_token = np.zeros((len(x),nmax,300))    \n",
    "    \n",
    "    if length == 'max':\n",
    "        nmax = max([len(word_tokenize(s)) for s in list_s]) \n",
    "        \n",
    "    for i,s in enumerate(list_s):\n",
    "        k=0\n",
    "        words_new = np.zeros((nmax,300)) \n",
    "        \n",
    "        words = str(s).lower()\n",
    "        words = word_tokenize(words)\n",
    "        \n",
    "        for w in words:\n",
    "            if k<nmax:\n",
    "                if w in glove.keys() and w not in stop_words:\n",
    "                    if w in tfidf_weight:\n",
    "                        words_new[k] = glove[w]*tfidf_weight[w] #Add tfidf weight to embedding\n",
    "                        glove[w]\n",
    "                        k += 1\n",
    "                    else:\n",
    "                        words_new[k] = glove[w] # do not add if not in tfidf (like '?')\n",
    "                        glove[w]\n",
    "                        k += 1                        \n",
    "                    \n",
    "        x_token[i] = words_new\n",
    "        \n",
    "    return x_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_tok = tokenize(xtrain)\n",
    "xtest_tok = tokenize(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SpatialDropout1D, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(150, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "900/900 [==============================] - 73s 81ms/step - loss: 0.9999 - acc: 0.5211 - val_loss: 1.0438 - val_acc: 0.4700\n",
      "Epoch 2/20\n",
      "900/900 [==============================] - 78s 87ms/step - loss: 0.9245 - acc: 0.6078 - val_loss: 0.9842 - val_acc: 0.5200\n",
      "Epoch 3/20\n",
      "900/900 [==============================] - 80s 89ms/step - loss: 0.8782 - acc: 0.6078 - val_loss: 1.0265 - val_acc: 0.5100\n",
      "Epoch 4/20\n",
      "900/900 [==============================] - 77s 85ms/step - loss: 0.8564 - acc: 0.6300 - val_loss: 1.0886 - val_acc: 0.4800\n",
      "Epoch 5/20\n",
      "900/900 [==============================] - 77s 86ms/step - loss: 0.8314 - acc: 0.6444 - val_loss: 1.0039 - val_acc: 0.5100\n",
      "Epoch 6/20\n",
      "900/900 [==============================] - 84s 93ms/step - loss: 0.7579 - acc: 0.6756 - val_loss: 1.0031 - val_acc: 0.5600\n",
      "Epoch 7/20\n",
      "900/900 [==============================] - 86s 96ms/step - loss: 0.7561 - acc: 0.6600 - val_loss: 1.0654 - val_acc: 0.4800\n",
      "Epoch 8/20\n",
      "900/900 [==============================] - 85s 94ms/step - loss: 0.7001 - acc: 0.7200 - val_loss: 1.1044 - val_acc: 0.4900\n",
      "Epoch 9/20\n",
      "900/900 [==============================] - 85s 94ms/step - loss: 0.6377 - acc: 0.7422 - val_loss: 1.1800 - val_acc: 0.5100\n",
      "Epoch 10/20\n",
      "900/900 [==============================] - 79s 88ms/step - loss: 0.6477 - acc: 0.7444 - val_loss: 1.1204 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "900/900 [==============================] - 79s 88ms/step - loss: 0.6577 - acc: 0.7222 - val_loss: 1.1397 - val_acc: 0.4900\n",
      "Epoch 12/20\n",
      "900/900 [==============================] - 77s 86ms/step - loss: 0.5570 - acc: 0.7733 - val_loss: 1.2257 - val_acc: 0.5100\n",
      "Epoch 13/20\n",
      "900/900 [==============================] - 78s 87ms/step - loss: 0.5419 - acc: 0.7922 - val_loss: 1.2406 - val_acc: 0.4500\n",
      "Epoch 14/20\n",
      "900/900 [==============================] - 77s 86ms/step - loss: 0.5071 - acc: 0.8067 - val_loss: 1.2427 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "900/900 [==============================] - 78s 86ms/step - loss: 0.4012 - acc: 0.8433 - val_loss: 1.4294 - val_acc: 0.4600\n",
      "Epoch 16/20\n",
      "900/900 [==============================] - 78s 87ms/step - loss: 0.4450 - acc: 0.8189 - val_loss: 1.3926 - val_acc: 0.5100\n",
      "Epoch 17/20\n",
      "900/900 [==============================] - 79s 88ms/step - loss: 0.4004 - acc: 0.8367 - val_loss: 1.3798 - val_acc: 0.4900\n",
      "Epoch 18/20\n",
      "900/900 [==============================] - 79s 88ms/step - loss: 0.3917 - acc: 0.8489 - val_loss: 1.2603 - val_acc: 0.5300\n",
      "Epoch 19/20\n",
      "900/900 [==============================] - 79s 88ms/step - loss: 0.3655 - acc: 0.8600 - val_loss: 1.3852 - val_acc: 0.5200\n",
      "Epoch 20/20\n",
      "900/900 [==============================] - 79s 88ms/step - loss: 0.2814 - acc: 0.8967 - val_loss: 1.7028 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b156c6a90>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_tok, y=ytrain_enc, \n",
    "          batch_size=16, \n",
    "          epochs=20,\n",
    "          validation_data=(xtest_tok,ytest_enc),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |                                    #           | 8391 Elapsed Time: 0:00:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8392/8392 [==============================] - 40s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "xsub = data_test['text']\n",
    "xsub_tok = tokenize(xsub)\n",
    "\n",
    "y_enc = model.predict(xsub_tok,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(y_enc,columns=data_sample.columns[1:])\n",
    "submission['id'] = data_sample['id']\n",
    "cols = submission.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "submission = submission[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.zip',index=False,compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf weighted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |    #                                            | 899 Elapsed Time: 0:00:00\n",
      "| |#                                                 | 99 Elapsed Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "xtrain_w_tok = tokenize_weighted(xtrain)\n",
    "xtest_w_tok = tokenize_weighted(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 1.1220 - val_loss: 1.0930\n",
      "Epoch 2/20\n",
      "900/900 [==============================] - 9s 9ms/step - loss: 1.0729 - val_loss: 1.0741\n",
      "Epoch 3/20\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 1.0178 - val_loss: 1.0769\n",
      "Epoch 4/20\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 0.9653 - val_loss: 1.0625\n",
      "Epoch 5/20\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 0.8684 - val_loss: 1.1392\n",
      "Epoch 6/20\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 0.8056 - val_loss: 1.2741\n",
      "Epoch 7/20\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 0.7374 - val_loss: 1.3848\n",
      "Epoch 8/20\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 0.6918 - val_loss: 1.3790\n",
      "Epoch 9/20\n",
      "900/900 [==============================] - 10s 11ms/step - loss: 0.6099 - val_loss: 1.4200\n",
      "Epoch 10/20\n",
      "900/900 [==============================] - 10s 11ms/step - loss: 0.5751 - val_loss: 1.4453\n",
      "Epoch 11/20\n",
      "900/900 [==============================] - 11s 12ms/step - loss: 0.5704 - val_loss: 1.4963\n",
      "Epoch 12/20\n",
      "900/900 [==============================] - 11s 12ms/step - loss: 0.4961 - val_loss: 1.5579\n",
      "Epoch 13/20\n",
      "900/900 [==============================] - 10s 11ms/step - loss: 0.4639 - val_loss: 1.5617\n",
      "Epoch 14/20\n",
      "900/900 [==============================] - 10s 11ms/step - loss: 0.4254 - val_loss: 1.6458\n",
      "Epoch 15/20\n",
      "900/900 [==============================] - 12s 13ms/step - loss: 0.3919 - val_loss: 1.7884\n",
      "Epoch 16/20\n",
      "900/900 [==============================] - 11s 12ms/step - loss: 0.3401 - val_loss: 1.8057\n",
      "Epoch 17/20\n",
      "900/900 [==============================] - 10s 11ms/step - loss: 0.3351 - val_loss: 1.9031\n",
      "Epoch 18/20\n",
      "900/900 [==============================] - 10s 11ms/step - loss: 0.2650 - val_loss: 2.1204\n",
      "Epoch 19/20\n",
      "900/900 [==============================] - 9s 11ms/step - loss: 0.2557 - val_loss: 2.1519\n",
      "Epoch 20/20\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 0.2507 - val_loss: 2.1549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa7150390>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SpatialDropout1D, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(xtrain_w_tok, y=ytrain_enc, \n",
    "          batch_size=300, \n",
    "          epochs=20,\n",
    "          validation_data=(xtest_w_tok,ytest_enc),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |                                       #        | 8391 Elapsed Time: 0:00:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8392/8392 [==============================] - 59s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "xsub = data_test['text']\n",
    "xsub_w_tok = tokenize(xsub)\n",
    "\n",
    "y_w_enc = model.predict(xsub_w_tok,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_w = pd.DataFrame(y_w_enc,columns=data_sample.columns[1:])\n",
    "submission_w['id'] = data_sample['id']\n",
    "cols = submission_w.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "submission_w = submission_w[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_w.to_csv('submission_w_norm.zip',index=False,compression='zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
